---
title: "Sales Analysis"
author: "Maeva Braeckevelt"
output:
  html_document:
    latex_engine: xelatex

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

During the Covid19 pandemic, every company had to adapt to the new measures. Restaurants were particularly challenged in order to stay profitable. Managers of restaurants struggle to staff their restaurant accordingly. I decided to analyze how certain variables affect the sales during this special period. As this situation is new,I collected the data from the first day of Belgium new lockdown regulation (20/10/2020) until november 30th, day I gather the data. As a robustness check, more data should be added, as the study serve only as a basis. The result for this particular restaurant are the following : first, the weather has little impact on the sales, but can be measured. Secondly, one additional productive hour corresponds to an average of 82.1 euro extra earnings when all the variables remain the same and thirdly, the sales change in function of the day of the week. Finally, I chose two models to predict the sales. However, with the two models chosen, I observed very high residuals. I would investigate those residuals and see if there were external problems, if not, I will wait to gather more data before giving those guidelines to the manager.

```{r, include=F, message=FALSE, echo=FALSE, eval=TRUE}
# Packages to use
rm(list=ls())
library(tidyverse)
library(lubridate)
#installed.packages("kableExtra")
library(knitr)
#install.packages("aTSA")
library(aTSA)
library(cowplot)
library(lspline)
library(estimatr)
library(texreg)
library(ggthemes)
library(geosphere)
library(moments)
library(dplyr)
library(pander)
library(jtools)
library(huxtable)

### Read the file
## Sales data
My_path <- "https://raw.githubusercontent.com/Maeva2408/DA2_assignment_2/main/Data/Raw/CA_cimdix_20oct_30nov.csv"
ca <- read_csv(paste0(My_path))

### Cleaning
##I will put the Downtime in hours
ca <- ca %>%  mutate(hours_of_downtime = Minutes_of_downtime/60)
ca$Minutes_of_downtime = NULL

##Reading time-format/converting to date_time
ca <- ca %>% mutate(Date = dmy(Date))

#The data is clean enough to start the analysis
```

# Introduction
What factors could impact the sales of a restaurant during the Covid-19 pandemic? That's the question I want to answer.
Restaurants are living a rough period : in Belgium, since october 20th of 2020, new sanitarian measures, among others,appeared : the restaurant can not welcome sitting clients anymore (only take-away meals) and there is a curfew at 10pm. The restaurants of my company are struggling to be profitable and their managers need as much information as possible to be able to take the right decision in a unstable environment. A way of staying profitable is to staff the restaurant according to the demand. Maybe knowing how the productive hours or the day of the week (and other variables) are correlated to the sales, in this particular period, could be insightful for this decision. I have taken the data of one of our restaurants and I will use it for my analysis. There is not such analysis done under "normal" cirucumstances yet but it would be interesting to compare those in the future.Due to the recentness of the situation I only have  42 observations, from october 20th to november 3Oth. Thus, the unlikely circumstances and the lack of observations may be problematic for significant result during the analysis and for external validity. Unfortunately, the actual circumstance will remain so the analysis is worth being done even with few observations. Moreover, once the model is set, adding observations and see the evolution will be interesting as well.

# Data
Staffing a restaurant according to the demand is a difficult decision. Providing statistical directives could be an advantage to the manager.There is a lot of variables that could affect the sales, I chose the variable in a way that either the manager has  control on the variables or that he can know it in advance to take it into consideration when he is doing the schedule.

##  Manager's decision : Working hours and hours of downtime
The manager can choose how many hours will be used per day and if he will be working on the shift as well. So, my goal is to observe how increasing or decreasing the productive hours could impact the sales and if the presence of the manager has an impact on the sales and how. I used the Sales in EURO (HT) per day (y). The productive hours (hours of kitchen's employee and waiters) as a variable and the leadership hours (hours of the managers working either in the kitchen or as a waiter) as another variable.
The restaurant is using an external delivery platform. When the team feels swamped by the number of clients on-site ordering in the restaurant, the team can chose to pause the delivery platform (downtime hours) so that no client can order on the platform during a given period. 
How does this method impact the sales?

```{r, eval=TRUE, echo=FALSE, results='asis'}

### Data

## Countinous Data


# Sales summary
summary_sales <- summarise(ca,
                                    variable = "Sales",
                                    n= n(),
                                    mean = mean(x = ca$Sales),
                                    median = median(x = ca$Sales),
                                    min= min(ca$Sales),
                                    max = max(ca$Sales),
                                    sd = sd(ca$Sales),
                                    skew = skewness(ca$Sales))
# Product hours summary
summary_prod <- summarise(ca,
                           variable = "Productive_hours",
                           n= n(),
                           mean = mean(x = ca$Productive_hours),
                           median = median(x = ca$Productive_hours),
                           min= min(ca$Productive_hours),
                           max = max(ca$Productive_hours),
                           sd = sd(ca$Productive_hours),
                           skew = skewness(ca$Productive_hours))

# leadership hours summary
summary_lead <- summarise(ca,
                          variable = "Leadership_hours",
                          n= n(),
                          mean = mean(x = ca$Leadership_hours),
                          median = median(x = ca$Leadership_hours),
                          min= min(ca$Leadership_hours),
                          max = max(ca$Leadership_hours),
                          sd = sd(ca$Leadership_hours),
                          skew = skewness(ca$Leadership_hours))

# Hours of downtime summary
summary_down <- summarise(ca,
                          variable = "hours_of_downtime",
                          n= n(),
                          mean = mean(x = ca$hours_of_downtime),
                          median = median(x = ca$hours_of_downtime),
                          min= min(ca$hours_of_downtime),
                          max = max(ca$hours_of_downtime),
                          sd = sd(ca$hours_of_downtime),
                          skew = skewness(ca$hours_of_downtime))

table_summary <- add_row(summary_sales,summary_prod)
table_summary <- table_summary %>% rbind(summary_lead, summary_down)
kable(table_summary)

# Visualization of the data:

# NO 1: check the time-series in different graps
ca_aux <- gather(ca, key = measure, value = Rate, 
                 c("Sales", "Productive_hours", "Leadership_hours", "hours_of_downtime"))

ggplot( ca_aux, aes(x = Date, y = Rate , color = measure ) ) + 
  geom_line() +
  facet_wrap( ~ measure , scales = "free",
              labeller = labeller( measure = c("Sales"="Sales (euro)",
             "Productive_hours"="Total of productive hours","Leadership_hours"="Total hours of leadership",
             "hours_of_downtime" = "Total hours od downtime" ) ) ) +
  labs( x = "Date" , y = '') +
  guides(color = FALSE ) +
  scale_y_continuous()+
  theme_bw()

```

I observe that for every variable the median and the mean are fairly close for each other. So the distribution
is close to symmetric. I chose not to transform any of ma variable.
This analysis is aim at helping manager, so it is essential to keep it the most simple as possible.
There is two extrem values, the 31/10/2020 there is a high total of hours of downtime and the 25/10/202
I have a fery few of observations and we are in a special period so drop or control some variables will loose the sense this analysis so I chose to keep all my data but keep that in mind during the analysis.

## Out of the control variable : day of the week and weather
In this analysis I choose to proceed as a stationnary time serie analysis.Indeed, there is a probability that the day of the week have a seasonality effect but with the few variable that I have, that will made my model more complicated and difficult to interpret and the difference will not be significative enough. (The PACF model help me to take that decision). However, with more observation it will be imperative to take lags and verify if there is a seasonality or if it was repressed by the pandemic. This two variables are nominal : weather can be sunny, cloudy and rainy and the day of the week can either Monday, Tuesday etc.

## Source of the data and transformation
I chose not to transform my data. This analysis aims at helping the managers so over-complicated interpretation could diminish its usefulnesst.The sales are coming from our POS (technology provider to the European hospitality market).The productive hours and the leadership hours are coming from our scheduler program. The downtime of the delivery platfrom is coming from the website of that supplier.The weather was compiled by myself looking into https://www.timeanddate.com/weather/belgium/brussels/historic website. I will chose a significative level of 0.2 because it is enough for guidelines.

```{r, include=F, message=FALSE, echo=FALSE, eval=TRUE}

# NO 2: standardization - good to compare the (co)-movement

stdd <- function( x ){ ( x - mean( x , rm.na = T ) ) / sd( x , na.rm = T ) }

ggplot( ca , aes( x = Date ) ) +
  geom_line( aes( y = stdd( Sales )    , color = "Sales" ) ) +
  geom_line( aes( y = stdd( Productive_hours ) , color = "Productive_hours" ) ) +
  geom_line( aes( y = stdd( hours_of_downtime )  , color = "hours_of_downtime" ) ) +
  geom_line( aes( y = stdd( Leadership_hours )    , color = "Leadership_hours") ) +
  scale_color_manual(name = "Variable",
                     values = c( "Sales" = "red", "Productive_hours" = "blue",
                                 "hours_of_downtime" = "orange", "Leadership_hours" = "green"),
                     labels = c("Sales" = "Sales (euro)", "Productive_hours"="Total of productive hours",
                                "hours_of_downtime"="Total hours of downtime", "Leadership_hours"="Total hours of leadership")) +
  labs(x="Years",y="Standardized values", title = "Visualization in standardize scale")


## no interpretation yet but good for visualization

### I will check if some variable are high correlated together
## Create a general function to check the pattern
chck_sp_ca <- function(x_var){
  ggplot( ca , aes(x = x_var, y = Sales)) +
    geom_point(color ='palevioletred3') +
    geom_smooth(method="loess" , formula = y ~ x, color = "lightgoldenrod2" )+
    labs(y = "Averaged Sales", title = "Visualization of the correlation between variables")+
    theme_bw()}


# Productive hours 
chck_sp_ca(ca$Productive_hours)
# Leadership hours
chck_sp_ca(ca$Leadership_hours)
# Hours of downtime
chck_sp_ca(ca$hours_of_downtime)

numeric_ca <- keep( ca , is.numeric )
ca_ct <- cor(numeric_ca , use = "complete.obs")

# Check for highly correlated values:
sum( ca_ct >= 0.8 & ca_ct != 1 ) / 2

## 0 : Good the variable are not correlated between each other

### I will do UNIT-ROOT TESTS to check the seasonality and trend

# Philips-Perron test for unit-root:
# Type 1: y_t = rho * y_t-1
# Type 2: y_t = alpha + rho * y_t-1
# Type 3: Delta y_t = alpha + delta * t + rho * Delta y_t-1 (Neglect this output!)
#   Reason to neglect: the power of this test is low (against e.g. seasonality)

## Sales
pp.test( ca$Sales , lag.short = F)

# interpretation : the p value for type 1 and 2 are higher than 0,05 therefore we can not reject the null, so there
# might be a trend or random walk

## Productive hours
pp.test( ca$Productive_hours, lag.short = F)
# interpretation : the p value for type 1 is higher than 0,05 therefore we can not reject the null, so there
# might be random walk

## Leadership hours
pp.test( ca$Leadership_hours , lag.short = F)
# P value is smaller than 0,05 so we can reject the null so there is no trend or random walk

## Hours of downtime
pp.test( ca$hours_of_downtime , lag.short = F)

# P value is smaller than 0,05 so we can reject the null that there is no trend or random walk


### I will try to visualize the ACF PACF Serial-correlation (a.k.a. Auto-correlation)



## In all of the graph I can see that nearly all the bars are inside the two lines.
## Only very few values are outside the lines (less that 5%), so the difference
## between a model with or without seasonality may not be significant and considering the
## few observation I have plus the fact that the interpretation are for the manager of the restaurant
## and need to be as simple as possible, I choose keep level and use my data as stationary.
## However, as soon as I add new observations to the data, it will be imperative to check again and use lags
## if necessary.


### Some scatter-plots:

## Sales and Leadership hours
ggplot( ca , aes( x = ca$Leadership_hours , y = ca$Sales ) ) +
  geom_point( color = 'palevioletred3' ) +
  geom_smooth(method="loess", color="lightgoldenrod2",se=F) +
  labs( x='Total hours of Leadership (un-adjusted)',y='Sales (euro)', 
        title = "Scatterplot Sales - Leadership hours per day of the week") +
  scale_y_continuous()+
  scale_x_continuous()+
  geom_text( aes(label = as.factor(ca$Day) ) , nudge_y = -0.15, size = 3 ) +
  theme_bw()

#  Low chances that leadership hours will have an impact on the sales, I will exclude this variable

## Sales and Productive hours
ggplot( ca , aes(x = ca$Productive_hours , y = ca$Sales ) ) +
  geom_point( color = 'palevioletred3' ) +
  geom_smooth(method="loess", color="lightgoldenrod2",se=F) +
  labs( x='Total of Productive hours (un-adjusted)',y='Sales (euro) (%)', title = "Scatterplot Sales - productive hours") +
  scale_y_continuous()+
  scale_x_continuous()+
  theme_bw()

# I observe that there are two cutting points. I can see that before 57 hours there is a linear regression,
# There is a plateau betwwen 57 and 60.5 where the sales stay pretty much the same
# after 60.5 there is again a linear regression. I will try a piecewise linear spline regression

# Piecewise linear spline regression
cutoff <- c(2,57,60.5)
ggplot( data = ca, aes( x = ca$Productive_hours  , y = ca$Sales ) ) + 
  geom_point( color='palevioletred3') +
  theme_bw()+
  geom_smooth( formula = y ~ lspline(x,cutoff) , method = lm , color = 'lightgoldenrod2' )+
  labs( x='Total of Productive hours (un-adjusted)',y='Sales (euro) (%)', title = "PLS : Sales and Productive hours")

# Let's compare to Linear

ggplot( data = ca, aes( x = ca$Productive_hours  , y = ca$Sales ) ) + 
  geom_point( color='palevioletred3') +
  theme_bw()+
  geom_smooth( method = lm , color = 'lightgoldenrod2' )+
  labs( x='Total of Productive hours (un-adjusted)',y='Sales (euro) (%)', 
        title = "Linear regression : Sales and Productive hours")

# I observe that the graph of the Piecewise linear spline regression is a bit overfitted 
# However, thet linear one is also ok so for the simplicity of the interpretation I will choose linear model


## Sales and Downtime hours
ggplot( ca , aes( x = ca$hours_of_downtime , y = ca$Sales ) ) +
  geom_point( color = 'palevioletred3' ) +
  geom_smooth(method="loess", color="lightgoldenrod2",se=F) +
  labs( x='Total of downtime hours (un-adjusted)',y=' Sales (euro)', 
        title = "Scatterplot : Sales and downtime hours per day of the week") +
  scale_y_continuous()+
  scale_x_continuous()+
  geom_text( aes(label = as.factor(ca$Day) ) , nudge_y = -0.15, size = 3 ) +
  theme_bw()

# By knowing the process of shut down of the delivery platform and my personal experience,
# I can say that the relation between sales and downtime hours is mediated by the number of client during the day 
# and the time of the day, both variable that I did not include in my data.
# From example, on a busy Friday evening, the restaurant needs to close the delivery platform for a few hours while
# they deal with the amount of customers on site.
# The number of orders and the time of the downtime could be found so, for the next analysis, it would be useful to include
#them, but for now I exclude the downtime hours of the analysis because the result will not be pertinent.

## Considering all the observations, I will model only three of my variables with linear regression.

### Models

## reg1 : Linear : Productive hours + Weather + Day of the week
## reg2 : Linear : Productive hours + Weather 
## reg3 : Linear : Productive hours + day of the week
## reg4 : Linear : Day of the week and weather

reg1 <- lm_robust( ca$Sales ~ ca$Productive_hours + as.factor( ca$Weather )+ as.factor(ca$Day) , data = ca )
summary(reg1)
reg2 <- lm_robust( ca$Sales ~ ca$Productive_hours + as.factor(ca$Weather) , data = ca )
summary(reg2)
reg3 <- lm_robust( ca$Sales ~ as.factor(ca$Day)  + ca$Productive_hours, data = ca )
summary(reg3)
reg4 <- lm_robust( ca$Sales ~ as.factor(ca$Day)+ as.factor(ca$Weather) ,  data = ca )
summary(reg4)


```

# Model choice and interpretation
After taking a look at the scatterplot, I realised that the leadership hours don't have a correlation with the Sales.
I also did not keep the downtime hours for the following reason : by knowing the process of the pause of the delivery platform and my personal experience, the relation between sales and downtime hours is mediated by the number of client during the day  and the time of the day, both variable that I did not include in my data. From example, on a busy Friday evening, the restaurant needs to close the delivery platform for a few hours while they deal with the amount of customers on site. The number of orders and the time of the downtime could be found so, for the next analysis, it would be useful to include it.
While I was looking at all the different models, I realized that even if there were no high correlation between the variables, the day of the week and the productive hours are related. It makes much sense because the manager changes the schedule according to the day of the week, taking the tendency pre-covid. It would be interesting to test a week with the same amount of productive hours everyday to detect a difference in the sales.I chose to keep two models, because I think they can both be valuable. I decided for the reason explained before to split days of the week and productive hours. The models are linear regression, indeed the following graph show a linear correlation.

Model 1 : **Formula :  Sales = 248.54 + 82.1 * Productive hours + 371.24* Sunny weather - 336.27* rainy weather**
          
**B0** : The interpretation is meaningless
**Beta 1** : One additional productive hour corresponds to an average of 82.1 euro extra earnings when all the variable remain the same.
**Beta 2** : On a sunny day, the sales change by 371 (euro) on average per day when the total hours of productivity remains the same.
**Beta_3** : The confidence for this interpretation is less than 80% so I won't interpret it.


What information can I use for the manager ?
The sales are a little bit higher on a sunny day, that's an interesting information to use in general.
Knowing how the productive hours impact the sales is very valuable. In the past, We already calculated how much a productive hours need to add if we want to stay profitable (no overstaffing) and keep the team in good working condition (not understaffed) and the result was 65 euro per hour. The new regulation, specially the take-away obligation, might allow the employees to be more productive (less cleaning, serving at the table, etc). It could be interesting to calculate again the ideal number in the new conditions and compare it to the 82.1 euro. Now with that information, the manager could estimate the sales with the hours of staffing and vice versa.

Model 2 : **Formula :  Sales = 6115.9 - 1407.9 * Monday - 301.9 * Saturday - 1392.3 * Sunday - 1054.7 * Thursday -857.8 * Tuesday -732.7 * Wednesday -324 * Rainy + 274* Sunny**

**B0** : The average Sales for a cloudy friday is 6115,9 euro.
**Beta 1** : On a Monday the average sales is 1407.9 euro less on average when the weather is cloudy.
**Beta 2** : The confidence for this interpretation is less than 80% so I wont interpret it.
**Beta 3** : 0n a Thursday the sales is 1392.3 euro less on average when the weather is cloudy.
**Beta 4** : On a Tuesday the sales is 857.8 euro less on average when the weather is cloudy.
**Beta 5** : On a Wednesday the sales is 732.7 euro less on average when the weather is cloudy.
**Beta 6** : The confidence for this interpretation is less than 80% so I won't interpret it.
**Beta 7** : On a sunny Friday, the sales changes by 274 (euro) on average per day.

What information can I provide to the manager ?
Same interpretation for the weather.
It would have make more sens to transform the variable and talk into percentage. The reason I didn't do it is operational : to create the schedule of the week, the manager has to insert the projected sales into our scheduler program and it calculates automatically the ratio of hours to use. So it is a lot easier for them to talk about average sales that about percentage.
Thanks to this formula I could create a pre-defined excel, to help them evaluate the sale on a sunny monday for example.


# Analysis of the residuals
```{r, include=F, message=FALSE, echo=FALSE, eval=TRUE}
#Model 1
ca$reg2_y_pred <- reg2$fitted.values
# Model 2
ca$reg4_y_pred <- reg4$fitted.values
# I Calculate the errors of the models

ca$reg2_res <- ca$Sales - ca$reg2_y_pred

ca$reg4_res <- ca$Sales - ca$reg4_y_pred

# Find date with largest negative errors
ca_res_neg2 <- ca %>% top_n( -5 , reg2_res ) %>% 
  select( Date , Sales , reg2_y_pred , reg2_res )

ca_res_neg4 <- ca %>% top_n( -5 , reg4_res ) %>% 
  select( Date , Sales , reg4_y_pred , reg4_res )

# Find date with largest positive errors
ca_res_pos2 <- ca %>% top_n( 5 , reg2_res ) %>% 
  select( Date , Sales , reg2_y_pred , reg2_res )

ca_res_pos4 <- ca %>% top_n( 5 , reg4_res ) %>% 
  select( Date , Sales , reg4_y_pred , reg4_res )


``` 
 

```{r results='asis', echo=FALSE}
  fintable <- cbind(ca_res_neg2,ca_res_neg4)
fintable %>% kable(align = "l", caption = "Modele 1 & 2 : List of the 5 sales with the largest negative errors
")
```
For both of the model, I observe high negative residuals, it means that the sales have been overestimated. This dates could be worth investigated for: if some external factors could have been a reason for a low sales on that day. Broken machines, a strike, an IT malfunction for example. I can also see that there are the two extreme value. By keeping them, I knew it will probably found it in the residuals.


```{r results='asis', echo=FALSE}
  fintable <- cbind(ca_res_pos2,ca_res_pos4)
fintable %>% kable(align = "l", caption = "Modele 1 & 2 : List of the 5 sales with the largest positive errors
")
``` 
For both model, I observe high positive residuals, it means that the sales have been underestimated. With the same logic, I would investigate those dates: if a big catering order has been made, if another restaurant had a problem and sent the client to this one, or if it was a evening before a bank holiday for example.

# Visualization of fit of the predicted value

```{r, echo=FALSE, fig.show = "hold", out.width = "50%", message=FALSE}
# Create: y_hat-y plot
## Modele 1
ggplot( data = ca ) +
  geom_point (aes( x = reg2_y_pred , y = Sales ) ,  color="palevioletred3")+
  geom_line( aes( x = Sales , y = Sales ) , color = "lightgoldenrod2" , size = 1.5 )+
  labs( x = "Predicted sales (euro)", y = "Actual Sales (euro)", 
        title = "Model 1 : Scatterplot : Actual value - Predicted value")

## Model 2
# Create: y_hat-y plot
ggplot( data = ca ) +
  geom_point (aes( x = reg4_y_pred , y = Sales ) ,  color="palevioletred3")+
  geom_line( aes( x = Sales , y = Sales ) , color = "lightgoldenrod2" , size = 1.5 )+
  labs( x = "Predicted sales (euro)", y = "Actual Sales (euro)", 
        title = "Model 2 : Scatterplot : Actual value - Predicted value")

```

The predicted values are not exactly on the line, but I can see that the tendency is no too far from it.
I can also say that the predicted value is more accurate when the sales are small for both of the models. 
The more observations we will add to the model, the more it will become accurate.

# Uncertainties in my prediction and plot: confidence interval of predicted values and prediction interval.

```{r, include=F, message=FALSE, echo=FALSE, eval=TRUE}
## Prediction uncertainty
###
# CI of regression line
# Model 1
pred2_CI <- predict( reg2, newdata = ca , interval ="confidence" , alpha = 0.2 )
pred2_CI
# Model 2
pred4_CI <- predict( reg4, newdata = ca , interval ="confidence" , alpha = 0.2 )
pred4_CI

# Hand made CI for regression line
# 1) Add to datatset:
ca <- ca %>% mutate( CI_reg2_lower = pred2_CI$fit[,2],
                     CI_reg2_upper = pred2_CI$fit[,3] )
ca <- ca %>% mutate( CI_reg4_lower = pred4_CI$fit[,2],
                     CI_reg4_upper = pred4_CI$fit[,3] )


``` 

```{r, echo=FALSE, fig.show = "hold", out.width = "50%", message=FALSE}
# 2) Plot

ggplot(  ) + 
  geom_point( data = ca, aes( x = Date, y = Sales ) , color='blue') +
  geom_line( data = ca, aes( x = Date, y = reg2_y_pred ) , color = 'red' , size = 1 ) +
  geom_line( data = ca, aes( x = Date, y = CI_reg2_lower ) , color = 'green' ,
             size = 1 , linetype = "dashed" ) +
  geom_line( data = ca, aes( x = Date, y = CI_reg2_upper ) , color = 'black' ,
             size = 1 , linetype = "dashed" ) +
  labs(x = "Dates",y = "Sales (euro)") 


ggplot(  ) + 
  geom_point( data = ca, aes( x = Date, y = Sales ) , color='blue') +
  geom_line( data = ca, aes( x = Date, y = reg4_y_pred ) , color = 'red' , size = 1 ) +
  geom_line( data = ca, aes( x = Date, y = CI_reg4_lower ) , color = 'green' ,
             size = 1 , linetype = "dashed" ) +
  geom_line( data = ca, aes( x = Date, y = CI_reg4_upper ) , color = 'black' ,
             size = 1 , linetype = "dashed" ) +
  labs(x = "Dates",y = "Sales (euro)") 



```

From my personal experience, a shift could be very difficult if the projection of sales was off by 500 (euro).
Between -500 and 500 some solutionS are provided : during the shift when there is less client than expected, we let people go home early, and deduct the work hours. If there are more clients than expected, employees know they are likely to work a bit 
longer. The step I would do is investigate those residuals and see if there were any external problems, if not,
I will wait to gather more data before giving those guidelines to the manager.


# External validity

As I mentionned before, because of the low amount of variable, I am not able to test my model. However, next month, with more data, I will be able to test this model and add lags for example if necessary.
I will keep the structure of this analysis for a basis for the other restaurant but I will definitely not apply those results to another restaurant of the company before testing and I know already, from personal experience that I will have to change the models.

# Summary

During the Covid19 pandemic, every company had to adapt to the new measures. Restaurants were particularly challenged in order to stay profitable. Managers of restaurants struggle to staff their restaurant accordingly. I decided to analyze how certain variables affect the sales during this special period. As this situation is new,I collected the data from the first day of Belgium new lockdown regulation (20/10/2020) until november 30th, day I gather the data. As a robustness check, more data should be added, as the study serve only as a basis. The result for this particular restaurant are the following : first, the weather has little impact on the sales, but can be measured. Secondly, one additional productive hour corresponds to an average of 82.1 euro extra earnings when all the variables remain the same and thirdly, the sales change in function of the day of the week. Finally, I chose two models to predict the sales. However, with the two models chosen, I observed very high residuals. I would investigate those residuals and see if there were external problems, if not, I will wait to gather more data before giving those guidelines to the manager.


\newpage

# Appendix

This appendix contains the documentation of the analysis.


## 1. Standardization - good to compare the (co)-movement

```{r, echo=FALSE, fig.show = "hold", out.width = "50%", message=FALSE}
# NO 2: standardization - good to compare the (co)-movement

stdd <- function( x ){ ( x - mean( x , rm.na = T ) ) / sd( x , na.rm = T ) }

ggplot( ca , aes( x = Date ) ) +
  geom_line( aes( y = stdd( ca$Productive_hours )    , color = "gdp" ) ) +
  geom_line( aes( y = stdd( ca$Leadership_hours ) , color = "inflat" ) ) +
  geom_line( aes( y = stdd( ca$hours_of_downtime )  , color = "unemp" ) ) +
  geom_line( aes( y = stdd( ca$Sales )  , color = "unemp" ) ) +
  scale_color_manual(name = "Variable",
                     values = c( "gdp" = "red", "inflat" = "blue",
                                 "unemp" = "orange", "EUR" = "green"),
                     labels = c("gdp" = "GDP", "inflat"="Inflation",
                                "unemp"="Unemployment", "EUR"="EUR")) +
  labs(x="Years",y="Standardized values")
```

## 2. Correlation between variables

```{r, echo=FALSE, fig.show = "hold", out.width = "50%", message=FALSE}
# Productive hours 
chck_sp_ca(ca$Productive_hours)
# Leadership hours
chck_sp_ca(ca$Leadership_hours)
# Hours of downtime
chck_sp_ca(ca$hours_of_downtime)


``` 

## 3. ACF PACF Serial-correlation (a.k.a. Auto-correlation)

```{r, include=F, message=FALSE, echo=FALSE, eval=TRUE, message=FALSE}
chck_sp_ca <- function(x_var){
  ggplot( ca , aes(x = x_var, y = Sales)) +
    geom_point(color ='palevioletred3') +
    geom_smooth(method="loess" , formula = y ~ x, color = "lightgoldenrod2" )+
    labs(y = "Averaged Sales", title = "Visualization of the correlation between variables")+
    theme_bw()}

# ACF PACF Serial-correlation (a.k.a. Auto-correlation)

url_git <- "https://raw.githubusercontent.com/Maeva2408/DA2_assignment_2/main/"

source( paste0( url_git , "Code/ggplot.acorr.R" ) )

```


```{r, echo=FALSE, fig.show = "hold", out.width = "50%", message=FALSE, warning=FALSE}

# Sales
ggplot.acorr( ca$Sales , lag.max = 24, ci= 0.95, 
              large.sample.size = F, horizontal = TRUE)

# productives hours
ggplot.acorr( ca$Productive_hours , lag.max = 24, ci= 0.95, 
              large.sample.size = F, horizontal = TRUE)

# Leadership
ggplot.acorr( ca$Leadership_hours , lag.max = 24, ci= 0.95, 
              large.sample.size = F, horizontal = TRUE)

# Hours of downtime
ggplot.acorr( ca$hours_of_downtime , lag.max = 24, ci= 0.95, 
              large.sample.size = F, horizontal = TRUE)


```

## 4. Scatterplot for investigation

```{r, echo=FALSE, fig.show = "hold", out.width = "50%", message=FALSE, warning=FALSE}
### Some scatter-plots:

## Sales and Leadership hours
ggplot( ca , aes( x = ca$Leadership_hours , y = ca$Sales ) ) +
  geom_point( color = 'palevioletred3' ) +
  geom_smooth(method="loess", color="lightgoldenrod2",se=F) +
  labs( x='Total hours of Leadership (un-adjusted)',y='Sales (euro)', 
        title = "Scatterplot Sales - Leadership hours per day of the week") +
  scale_y_continuous()+
  scale_x_continuous()+
  geom_text( aes(label = as.factor(ca$Day) ) , nudge_y = -0.15, size = 3 ) +
  theme_bw()

#  Low chances that leadership hours will have an impact on the sales, I will exclude this variable

## Sales and Productive hours
ggplot( ca , aes(x = ca$Productive_hours , y = ca$Sales ) ) +
  geom_point( color = 'palevioletred3' ) +
  geom_smooth(method="loess", color="lightgoldenrod2",se=F) +
  labs( x='Total of Productive hours (un-adjusted)',y='Sales (euro) (%)', title = "Scatterplot Sales - productive hours") +
  scale_y_continuous()+
  scale_x_continuous()+
  theme_bw()

# I observe that there are two cutting points. I can see that before 57 hours there is a linear regression,
# There is a plateau betwwen 57 and 60.5 where the sales stay pretty much the same
# after 60.5 there is again a linear regression. I will try a piecewise linear spline regression

# Piecewise linear spline regression
cutoff <- c(2,57,60.5)
ggplot( data = ca, aes( x = ca$Productive_hours  , y = ca$Sales ) ) + 
  geom_point( color='palevioletred3') +
  theme_bw()+
  geom_smooth( formula = y ~ lspline(x,cutoff) , method = lm , color = 'lightgoldenrod2' )+
  labs( x='Total of Productive hours (un-adjusted)',y='Sales (euro) (%)', title = "PLS : Sales and Productive hours")

# Let's compare to Linear

ggplot( data = ca, aes( x = ca$Productive_hours  , y = ca$Sales ) ) + 
  geom_point( color='palevioletred3') +
  theme_bw()+
  geom_smooth( method = lm , color = 'lightgoldenrod2' )+
  labs( x='Total of Productive hours (un-adjusted)',y='Sales (euro) (%)', 
        title = "Linear regression : Sales and Productive hours")

# I observe that the graph of the Piecewise linear spline regression is a bit overfitted 
# However, thet linear one is also ok so for the simplicity of the interpretation I will choose linear model


## Sales and Downtime hours
ggplot( ca , aes( x = ca$hours_of_downtime , y = ca$Sales ) ) +
  geom_point( color = 'palevioletred3' ) +
  geom_smooth(method="loess", color="lightgoldenrod2",se=F) +
  labs( x='Total of downtime hours (un-adjusted)',y=' Sales (euro)', 
        title = "Scatterplot : Sales and downtime hours per day of the week") +
  scale_y_continuous()+
  scale_x_continuous()+
  geom_text( aes(label = as.factor(ca$Day) ) , nudge_y = -0.15, size = 3 ) +
  theme_bw()


```


